---
title: "Bioinformatics and Stats Tutorials"
authors:
  - name: "Alicia M. Rich, Ph.D."
    affiliation: "Rich Lab, University of Nebraska Omaha"
description: |
  R markdown tutorials for Rich Lab bioinformatic workflows
output:
  html_document:
    theme:
      bslib: true
    toc: true
    toc_depth: 3
    css: journal.css
site: distill::distill_website

---

```{r setup, include = F}
knitr::opts_chunk$set(message = FALSE,
               warning = FALSE,
               echo    = FALSE,
               include = TRUE,
               eval    = TRUE,
               comment = "")

library(tidyverse)
library(bslib)
library(htmltools)
library(downloadthis)
```


This repository contains the scripts, tutorials, and templates for the Rich Lab's mainstream bioinformatic workflows. You can find some of the current recommended tutorial versions of these workflows linked below.

# Tutorial Options

##  [`MetadataSetup.html`](https://rich-molecular-health-lab.github.io/tutorials/MetadataSetup.html)

- The purpose of a metadata file is to organize the potential independent or predictor variables for your analysis into a single table with one row per SampleID. Then, when you produce a set of potential dependent or outcome values, you organize those into a similar structure with the SampleIDs organized rowwise to streamline the process of matching predictor variables to outcome variables by SampleID.

- It's good practice to keep as much of your information in one tidy table as possible so that you can keep pulling from that source to further filter, wrangle and analyze without losing track of different versions and datasets over time. 

- That means you should brainstorm as many possible predictor variables you might use in downstream analysis as possible and organize them into one tidy table where each SampleID is matched to a value for every variable. *You will end up ignoring most of these variables as you construct individual tests and visuals later, so consider this simply a rough draft of your information of interest.*

- This tutorial uses the Pygmy Loris dataset as an example. You may have a very different set of variables to organize for your own project.

### Files Needed

1.  `compilation_loris.tsv` - *produced by the `SampleInventory.Rmd` script*
2.  **All other files depend on your specific variables of interest**
   - *Mine mostly focus on diet trials and environmental context for the Pygmy Loris subjects*
   
```{r}

```

   
### Files Produced

1.  `samples_metadata.tsv` - This file gets carried forward on several other scripts in this repository to help you build consistent predictor variables across datasets.

##  [`microbiome_new_data.html`](https://rich-molecular-health-lab.github.io/bioinformatics_stats/microbiome_new_data.html)

- Follow this tutorial to take new raw read alignment data produced by Epi2ME Lab's `wf-16s` pipeline and convert it into fully formatted, tidy tables that you can normalize and analyze using `microeco` or a similar package.

### Files Needed

1.  `samples_metadata.tsv` - *produced by the `MetadataSetup.Rmd` script*
2.  `*_abundance_table_species.tsv` - *produced by the `wf-16s` pipeline*
3.  `*_wf-16s-report.html` - *produced by the `wf-16s` pipeline*

  - The tutorial provides instructions and code to download one `.csv` file per demultiplexed sample from this `.html` file for a given sequencing run and merge those into a table with raw alignment data
  
### Files Produced

1.  `representative_seqs.fasta` - This contains one GenBank reference sequence for each taxon in your dataset that we can use to substitute for ASVs used in many approaches designed for short read 16S data.
2.  `representative_seqs_tax4fun.fasta` - This is the same as the previous file, except with different syntax to work with the `Tax4Fun2` package.
3.   `tree.newick` - This is a phylogenetic tree produced from the previous fasta multiple sequence alignment.
4.  `taxonomy_table.tsv` - This table is required for `microeco` and several other metagenomic packages. It must contain exactly one row per taxon in your dataset.
5.  `otu_table.tsv` - This table is required for `microeco` and several other metagenomic packages. It must contain exactly one row per taxon and one column per sample in your dataset. The numeric values represent the raw count of reads mapped to a given taxon within a given sample. We will want to normalize these before analyzing/interpreting them.
6.  `sample_table.tsv` - This table is required for `microeco` and several other metagenomic packages. It must contain exactly one row per sample in your dataset. The columns contain metadata values that we may use as independent/predictor variables for interpreting our results.

# General Script Advice

## On Your First-Use of a Workflow

I created a secondary repository called [`workflows_first_use`](https://github.com/Rich-Molecular-Health-Lab/workflows_first_use). This is where I will store some scripts/tutorials with specific instructions for setting up your working directory and environment for specialized packages (especially for `microeco`).

## `params` option in the R Markdown Header

Here is what my default `yaml` header on most R Markdown documents in this repository look like:

```{r, echo = FALSE, results='asis'}
card(tagList(tags$pre(includeText("setup/header_default.txt"))))
```

I use the `params` option for streamlining some automation across different scripts. You can use the sampleset setting under params in the header of this script to select which sampleset you will be working with. Everywhere in a chunk of code where I have written `params$sampleset` will be replaced with whichever strong you change your params value to in the header.

## File Paths and the `config` Package

I also use `config` for streamlining and organization. If you want to do the same, you should create your own `config.yml` file in your project working directory. You can expand the content below to see what my example `config.yml` file looks like as well as the R script I use to integrate this with the `params` values for consistent file sourcing and writing across datasets.

```{r, echo = FALSE}
page_fluid(
    accordion(
      open = FALSE,
      accordion_panel(
        "Show/Hide `config.yml` File",
        tagList(tags$pre(includeText("config.yml")))
    ),
      accordion_panel(
        "Show/Hide `config_paths.R` Script",
        tagList(tags$pre(includeText("setup/config_paths.R")))
    )
  )
)
```

## Custom Engines

I use custom language engines in some scripts that I named "terminal" and "bash". If you see a chunk of code with {terminal, warning = FALSE} written where you would usually see {r} at the top of the chunk, then running the chunk should only print that code as a text string in this document. This just makes it easier for me to copy and paste the code directly into the terminal panel that I use in my R Studio window when running code through a remote server instead of my local R console. There are ways to set R Studio up to run code through multiple servers, but I find this the simplest way to switch back and forth while still keeping a record of the code that have used or changes I have made to it.  
  
You can expand the content below to see the code that I run at the start of many of these scripts to initiate those language engines.

```{r, echo = FALSE}
page_fluid(
    accordion(
      open = FALSE,
      accordion_panel(
        "knit_engines.R",
        tagList(tags$pre(includeText("setup/knit_engines.R")))
    )
  )
)
```






